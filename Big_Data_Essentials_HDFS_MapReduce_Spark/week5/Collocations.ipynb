{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log\n",
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext(conf=SparkConf().setAppName(\"MyApp\").setMaster(\"local\"))\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_article(line):\n",
    "    try:\n",
    "        article_id, text = unicode(line.rstrip()).split('\\t', 1)\n",
    "        text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "        words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "        return [w.lower() for w in words]\n",
    "    except ValueError as e:\n",
    "        return []\n",
    "\n",
    "wiki = sc.textFile(\"/data/wiki/en_articles_part/articles-part\", 16).map(parse_article)\n",
    "stop_words = open(\"/datasets/stop_words_en.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the word counts and find their probability of occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_counts = wiki.flatMap(lambda x: x[1:]).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the stop words\n",
    "wiki_counts = wiki_counts.filter(lambda x: x[0] not in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = wiki_counts.map(lambda x: x[1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_prob = wiki_counts.map(lambda x: (x[0], float(x[1]) / n_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the bigrams and their probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_bigrams = wiki.flatMap(lambda x: zip(x[1:], x[2:]+[''])).map(lambda x: (x[0]+'_'+x[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = wiki_bigrams.reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bigrams = bigram_counts.map(lambda x: x[1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts_limited = bigram_counts.filter(lambda x: x[1] >= 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_prob = bigram_counts_limited.map(lambda x: (x[0], float(x[1])/n_bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the NPMI for the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the bigrams into their words\n",
    "bigram_split = bigram_prob.map(lambda x: (x[0], x[0].split('_')[0], x[0].split('_')[1], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the probabilities of each word occuring (I would love to see a cleaner way to do this...)\n",
    "# Note that the final order is (bigram a_b, P(ab), P(a), P(b))\n",
    "all_probs = bigram_split.map(lambda x: (x[1], (x[0], x[2], x[3]))).join(words_prob)\n",
    "all_probs = all_probs.map(lambda x: (x[1][0][1], (x[1][0][0], x[1][0][2], x[1][1]))).join(words_prob)\n",
    "all_probs = all_probs.map(lambda x: (x[1][0][0], x[1][0][1], x[1][0][2], x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can caluclate the NPMI for each row\n",
    "npmi = all_probs.map(lambda x: (-log(x[1] / (x[2]*x[3]))/x[1], x[0])).sortByKey(ascending=False).map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'united_states',\n",
       " u'new_york',\n",
       " u'external_links',\n",
       " u'university_press',\n",
       " u'united_kingdom',\n",
       " u'american_actor',\n",
       " u'references_external',\n",
       " u'19th_century',\n",
       " u'air_force',\n",
       " u'20th_century',\n",
       " u'years_later',\n",
       " u'american_actress',\n",
       " u'north_american',\n",
       " u'north_america',\n",
       " u'york_city',\n",
       " u'new_zealand',\n",
       " u'prime_minister',\n",
       " u'american_baseball',\n",
       " u'soviet_union',\n",
       " u'united_nations',\n",
       " u'high_school',\n",
       " u'american_football',\n",
       " u'baseball_player',\n",
       " u'american_singer-songwriter',\n",
       " u'south_africa',\n",
       " u'catholic_church',\n",
       " u'roman_catholic',\n",
       " u'notes_references',\n",
       " u'took_place',\n",
       " u'roman_empire',\n",
       " u'supreme_court',\n",
       " u'los_angeles',\n",
       " u'san_francisco']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npmi.take(39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
